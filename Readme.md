# ğŸ­ Multi-Modal Emotion Recognition (Audio + Text)

This project implements **emotion recognition** from both **audio (speech)** and **text**, using classical machine learning and deep learning approaches.

---

## ğŸ”Š Audio Pipeline
**Workflow:** Speech â†’ Feature Extraction â†’ ML Classifiers

- **Feature Extraction:** Using [Librosa](https://librosa.org/)
  - MFCCs + Delta + Delta-Delta
  - Chroma
  - Spectral features (Centroid, Bandwidth, Contrast, Rolloff)
  - RMS Energy & Pitch statistics
- **Data Processing:**
  - Organizes EMO-DB and IEMOCAP datasets into `processed_data/` by emotion
  - Data augmentation: pitch shift, time-stretch, noise
  - Handles class imbalance with SMOTE
- **Classifiers Trained:**
  - MLP, Stacking, Voting, XGBoost
- **Evaluation:**
  - Confusion matrices
  - Classification reports
- **Outputs:**
  - Trained model, scaler, label encoder saved as `audio_model_stacking.pkl`
  - Confusion matrices: `confusion_matrix_MLP.png`, `confusion_matrix_Stacking.png`, etc.

---

## ğŸ“ Text Pipeline
**Workflow:** Text â†’ Tokenization â†’ Deep Learning Models â†’ Multi-label Emotion Prediction

- Supports **multi-label emotion classification**
- Models used:
  - BERT (fine-tuned)
  - BiLSTM with embeddings & padding
- **Evaluation Metrics:** Accuracy, F1 (micro/macro), Precision, Recall
- Supports **interactive inference**:
  ```text
  Enter a sentence (or 'quit'): I am so happy today!
  BERT Prediction: ['joy', 'excitement']
  BiLSTM Prediction: ['happiness', 'excitement']

Models saved as:

-bert_emotion_model/

-bilstm_emotion_model.pt

ğŸ“‚ Project Structure
.
â”œâ”€â”€ Datasets/
â”‚   â”œâ”€â”€ EMO-DB/wav/           # Raw EMO-DB audio files
â”‚   â””â”€â”€ IEMOCAP/
â”‚       â”œâ”€â”€ IEMOCAP_audio/    # IEMOCAP audio files
â”‚       â””â”€â”€ iemocapTrans.csv  # Metadata CSV
â”œâ”€â”€ processed_data/           # Organized audio files by emotion
â”œâ”€â”€ audio_train.py            # Audio pipeline (ML)
â”œâ”€â”€ main.py                   # Text pipeline (BERT + BiLSTM)
â”œâ”€â”€ bert_local/               # Local BERT checkpoint
â”œâ”€â”€ results/                  # Output from Trainer
â”œâ”€â”€ audio_model_stacking.pkl  # Trained audio ML model
â”œâ”€â”€ bilstm_emotion_model.pt   # Trained BiLSTM model
â”œâ”€â”€ bert_emotion_model/       # Saved BERT model
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md

ğŸš€ Installation
git clone [https://github.com/your-username/multimodal-emotion-recognition.git](https://github.com/RohanRaghav/Emotion_Detection.git)
cd multimodal-emotion-recognition
python -m venv .venv
# Activate environment
# Linux / Mac
source .venv/bin/activate
# Windows
.venv\Scripts\activate
pip install -r requirements.txt

ğŸ›  Usage
1ï¸âƒ£ Audio Pipeline
python audio_emotion_recognition.py
Organizes datasets into processed_data/

Extracts features, trains classifiers

Saves audio_model_stacking.pkl

Generates confusion matrices

Inference Example:
import joblib

stack_clf, le, scaler = joblib.load("audio_model_stacking.pkl")
# Extract features from new audio file
# Predict using stack_clf
2ï¸âƒ£ Text Pipeline
python text_emotion_main.py

Trains BERT & BiLSTM models

Saves bert_emotion_model/ and bilstm_emotion_model.pt
Inference Example:
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load BERT
tokenizer = BertTokenizer.from_pretrained("bert_emotion_model")
model = BertForSequenceClassification.from_pretrained("bert_emotion_model")
# Run predict_with_bert(text)

âš ï¸ Notes

Audio pipeline requires EMO-DB or IEMOCAP datasets.

Text pipeline requires test.tsv with columns: text, label, id.

GPU is recommended for BERT/BiLSTM training.

Multi-label text classification uses sigmoid + BCEWithLogitsLoss.

ğŸ“Š Results

Audio Models: Evaluated with confusion matrices & classification reports

Text Models: Evaluated with accuracy, F1-micro, F1-macro

ğŸ‘¨â€ğŸ’» Author

Rohan Raghav â€“ Full Stack Developer & Machine Learning Enthusiast

Organized hackathons & AI events

Projects: Speech & Text Emotion Recognition, NLP, ML & Deep Learning